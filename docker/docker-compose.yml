version: "3.9"

services:
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    image: eka-base:latest
    container_name: fastapi_app
    ports:
      - "8001:8001"
    depends_on:
      - redis
      - chroma
      - mlflow
    environment:
      CHROMA_API_IMPL: "rest"
      CHROMA_SERVER_HOST: "chroma"
      CHROMA_SERVER_PORT: 8000
      REDIS_HOST: "redis"
      REDIS_PORT: 6379
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
    command: uvicorn src.api.main:app --host 0.0.0.0 --port 8001
    volumes:
    - ../:/app

  streamlit:
    image: eka-base:latest
    container_name: streamlit_ui
    ports:
      - "8501:8501"
    depends_on:
      - api
    command: streamlit run ui/chat_app.py --server.address 0.0.0.0 --server.port 8501
    volumes:
      - ../:/app

  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"

  chroma:
    image: chromadb/chroma:latest
    container_name: chroma
    ports:
      - "8000:8000"
    environment:
      CHROMA_DB_IMPL: "duckdb+parquet"
      CHROMA_SERVER_AUTH_ENABLED: "false"
    volumes:
      - ../data/chroma_db:/data

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ../src/config/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

  # Option 1: MLflow (Local SQLite Backend)
  # Uncomment this section if you want to use a local SQLite backend
  # for development or single-user use.
  # mlflow:
  #   image: ghcr.io/mlflow/mlflow:v2.14.1
  #   container_name: mlflow
  #   ports:
  #     - "5000:5000"
  #   environment:
  #     MLFLOW_BACKEND_STORE_URI: sqlite:///mlflow.db
  #     MLFLOW_DEFAULT_ARTIFACT_ROOT: /mlruns
  #   volumes:
  #     - ../mlruns:/mlruns
  #     - ../mlflow.db:/mlflow.db
  #   command: >
  #     mlflow server
  #     --backend-store-uri sqlite:///mlflow.db
  #     --default-artifact-root /mlruns
  #     --host 0.0.0.0
  #     --port 5000

  # Option 2: MLflow (Production: PostgreSQL Backend)
  postgres:
    image: postgres:15
    container_name: mlflow_postgres
    environment:
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow
      POSTGRES_DB: mlflow
    ports:
      - "5432:5432"
    volumes:
      - ../data/postgres:/var/lib/postgresql/data

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.14.1
    container_name: mlflow
    depends_on:
      - postgres
    environment:
      MLFLOW_BACKEND_STORE_URI: postgresql://mlflow:mlflow@postgres:5432/mlflow
      MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://your-bucket-name/mlflow-artifacts
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
    ports:
      - "5000:5000"
    command: >
      mlflow server
      --backend-store-uri postgresql://mlflow:mlflow@postgres:5432/mlflow
      --default-artifact-root s3://your-bucket-name/mlflow-artifacts
      --host 0.0.0.0
      --port 5000


  # Network (Optional)
  # Docker Compose creates a default network automatically,
  # but you can explicitly define one if you prefer.
  #
  # networks:
  #   default:
  #     name: enterprise_ai_network
  #     driver: bridge
